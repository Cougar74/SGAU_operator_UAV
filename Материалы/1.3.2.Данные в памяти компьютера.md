---
tags: 
Создана: "2025-10-28"
---
# Данные в памяти компьютера
Мы часто говорим: «В компьютере всё состоит из нулей и единиц». Но что это значит на практике? Как буква, число или цвет картинки превращаются в эту последовательность и живут в памяти?
## Оперативная память
**Оперативная память** (ОЗУ – Оперативное Запоминающее Устройство) служит для оперативной записи, хранения и считывания информации (программ и данных) в процессе ее обработки. ОЗУ – энергозависимая память, т.е. при отключении питания информация в ней теряется.
Когда мы запускаем программу или открываем файл, процессор с жесткого диска копирует информацию в ОЗУ, чтобы процессор мог с ним быстро работать. После выключения компьютера всё содержимое ОЗУ стирается.

Физически ОЗУ – это множество ячеек, состоящих из конденсаторов и транзисторов. Если говорить упрощенно, то если конденсатор **заряжен** – это соответствует 1, а если конденсатор **разряжен** – 0. Таким образом, каждая ячейка физически может хранить один **бит** информации.
**Бит** (binary digit – двоичная цифра) – это наименьшая единица информации. Он может принимать только одно из двух значений: **0** или **1**.
**Байт** – это группа из **8 битов**. Байт является основной адресуемой единицей памяти в большинстве современных компьютеров.
**Пример:**  
`01010101` – это один байт.  
`11110000` – это тоже один байт.

С помощью одного байта можно закодировать $2^8 = 256$ различных значений (от 0 до 255). Объем памяти измеряется в килобайтах (**КБ**), мегабайтах (**МБ**), гигабайтах (**ГБ**) и т.д.
## Системы счисления
Чтобы понимать представление чисел, нужно знать две счисления.
Под **системой счисления** понимается способ записи чисел с помощью символов (цифр, букв и т.д.). Системы счисления бывают **позиционные** и **непозиционные**.
**Непозиционной** является, например, римская система счисления. В **позиционных** системах счисления любое число записывается в виде последовательности символов, количественное значение («вес») которых зависит от местоположения в числе, т.е. позиции в записи числа.
**Основанием** позиционной системы счисления называется целое число, определяющее количество символов, используемых в ней (обозначим его через $p$). В позиционной системе счисления с основанием $p$ любое число R может быть представлено в виде: $$R_{(p)} = A_{N}p^{N} + A_{N-1}p^{N-1} + ... + a_{1}p^{1} + a_{0}p^{0} + a_{-1}p^{-1} + ... + a_{-K}p^{-K}$$, где коэффициенты $a_i$ - символы в изображении числа $R$, которые принимают значения от 0 до $p-1$.
Например, $$265.23_{(10)} = 2*10^2 + 6*10^1 + 5*10^0 + 2*10^{-1} + 3*10^{-2}$$
В вычислительной технике широко используются позиционные системы счисления (двоичная, восьмеричная, десятичная, шестнадцатеричная). В памяти компьютера, на уровне аппаратной реализации, информация представляется в двоичной системе счисления.
### Двоичная система счисления
В двоичной системе счисления используются две – цифры **0** и **1**.
 Таблицы операций в двоичной системе счисления:

| Сложение | 0   | 1   |     | Умножение | 0   | 1   |
| -------- | --- | --- | --- | --------- | --- | --- |
| +        | 0   | 1   |     | ×         | 0   | 1   |
| 0        | 0   | 1   |     | 0         | 0   | 0   |
| 1        | 1   | 10  |     | 1         | 0   | 1   |

Арифметические операции в двоичной системе счисления выполняются по тем же правилам, что и в десятичной системе счисления. Приведем некоторые примеры выполнения основных операций в двоичной системе счисления.
![[Pasted image 20251109000033.png]]
### Шестнадцатеричная система счисления
В шестнадцатеричной системе счисления используются цифры **0**, **1**, **2**, **3**, **4**, **5**, **6**, **7**, **8**, **9** и буквы **A**, **B**, **C**, **D**, **E**, **F**.

| +   | 1   | 2   | 3   | 4   | 5   | 6   | 7   | 8   | 9   | A   | B   | C   | D   | E   | F   |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 1   | 2   | 3   | 4   | 5   | 6   | 7   | 8   | 9   | A   | B   | C   | D   | E   | F   | 10  |
| 2   | 3   | 4   | 5   | 6   | 7   | 8   | 9   | A   | B   | C   | D   | E   | F   | 10  | 11  |
| 3   | 4   | 5   | 6   | 7   | 8   | 9   | A   | B   | C   | D   | E   | F   | 10  | 11  | 12  |
| 4   | 5   | 6   | 7   | 8   | 9   | A   | B   | C   | D   | E   | F   | 10  | 11  | 12  | 13  |
| 5   | 6   | 7   | 8   | 9   | A   | B   | C   | D   | E   | F   | 10  | 11  | 12  | 13  | 14  |
| 6   | 7   | 8   | 9   | A   | B   | C   | D   | E   | F   | 10  | 11  | 12  | 13  | 14  | 15  |
| 7   | 8   | 9   | A   | B   | C   | D   | E   | F   | 10  | 11  | 12  | 13  | 14  | 15  | 16  |
| 8   | 9   | A   | B   | C   | D   | E   | F   | 10  | 11  | 12  | 13  | 14  | 15  | 16  | 17  |
| 9   | A   | B   | C   | D   | E   | F   | 10  | 11  | 12  | 13  | 14  | 15  | 16  | 17  | 18  |
| A   | B   | C   | D   | E   | F   | 10  | 11  | 12  | 13  | 14  | 15  | 16  | 17  | 18  | 19  |
| B   | C   | D   | E   | F   | 10  | 11  | 12  | 13  | 14  | 15  | 16  | 17  | 18  | 19  | 1A  |
| C   | D   | E   | F   | 10  | 11  | 12  | 13  | 14  | 15  | 16  | 17  | 18  | 19  | 1A  | 1B  |
| D   | E   | F   | 10  | 11  | 12  | 13  | 14  | 15  | 16  | 17  | 18  | 19  | 1A  | 1B  | 1C  |
| E   | F   | 10  | 11  | 12  | 13  | 14  | 15  | 16  | 17  | 18  | 19  | 1A  | 1B  | 1C  | 1D  |
| F   | 10  | 11  | 12  | 13  | 14  | 15  | 16  | 17  | 18  | 19  | 1A  | 1B  | 1C  | 1D  | 1E  |

| x   | 1   | 2   | 3   | 4   | 5   | 6   | 7   | 8   | 9   | A   | B   | C   | D   | E   | F   |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 1   | 1   | 2   | 3   | 4   | 5   | 6   | 7   | 8   | 9   | A   | B   | C   | D   | E   | F   |
| 2   | 2   | 4   | 6   | 8   | A   | C   | E   | 10  | 12  | 14  | 16  | 18  | 1A  | 1C  | 1E  |
| 3   | 3   | 6   | 9   | C   | F   | 12  | 15  | 18  | 1B  | 1E  | 21  | 24  | 27  | 2A  | 2D  |
| 4   | 4   | 8   | C   | 10  | 14  | 18  | 1C  | 20  | 24  | 28  | 2C  | 30  | 34  | 38  | 3C  |
| 5   | 5   | A   | F   | 14  | 19  | 1E  | 23  | 28  | 2D  | 32  | 37  | 3C  | 41  | 46  | 4B  |
| 6   | 6   | C   | 12  | 18  | 1E  | 24  | 2A  | 30  | 36  | 3C  | 42  | 48  | 4E  | 54  | 5A  |
| 7   | 7   | E   | 15  | 1C  | 23  | 2A  | 31  | 38  | 3F  | 46  | 4D  | 54  | 5B  | 62  | 69  |
| 8   | 8   | 10  | 18  | 20  | 28  | 30  | 38  | 40  | 48  | 50  | 58  | 60  | 68  | 70  | 78  |
| 9   | 9   | 12  | 1B  | 24  | 2D  | 36  | 3F  | 48  | 51  | 5A  | 63  | 6C  | 75  | 7E  | 87  |
| A   | A   | 14  | 1E  | 28  | 32  | 3C  | 46  | 50  | 5A  | 64  | 6E  | 78  | 82  | 8C  | 96  |
| B   | B   | 16  | 21  | 2C  | 37  | 42  | 4D  | 58  | 63  | 6E  | 79  | 84  | 8F  | 9A  | A5  |
| C   | C   | 18  | 24  | 30  | 3C  | 48  | 54  | 60  | 6C  | 78  | 84  | 90  | 9C  | A8  | B4  |
| D   | D   | 1A  | 27  | 34  | 41  | 4E  | 5B  | 68  | 75  | 82  | 8F  | 9C  | A9  | B6  | C3  |
| E   | E   | 1C  | 2A  | 38  | 46  | 54  | 62  | 70  | 7E  | 8C  | 9A  | A8  | B6  | C4  | D2  |
| F   | F   | 1E  | 2D  | 3C  | 4B  | 5A  | 69  | 78  | 87  | 96  | A5  | B4  | C3  | D2  | E1  |

Приведем некоторые примеры выполнения основных операций в шестнадцатеричной системе счисления.
![[Pasted image 20251109000543.png]]
## Числа в различных системах счисления

| Десятичное число | Двоичное число | Восьмеричное число | Шестнадцатеричное число |
| ---------------- | -------------- | ------------------ | ----------------------- |
| 0                | 0000           | 0                  | 0                       |
| 1                | 0001           | 1                  | 1                       |
| 2                | 0010           | 2                  | 2                       |
| 3                | 0011           | 3                  | 3                       |
| 4                | 0100           | 4                  | 4                       |
| 5                | 0101           | 5                  | 5                       |
| 6                | 0110           | 6                  | 6                       |
| 7                | 0111           | 7                  | 7                       |
| 8                | 1000           | 10                 | 8                       |
| 9                | 1001           | 11                 | 9                       |
| 10               | 1010           | 12                 | A                       |
| 11               | 1011           | 13                 | B                       |
| 12               | 1100           | 14                 | C                       |
| 13               | 1101           | 15                 | D                       |
| 14               | 1110           | 16                 | E                       |
| 15               | 1111           | 17                 | F                       |
| 16               | 10000          | 20                 | 10                      |
| 20               | 10100          | 24                 | 14                      |
| 100              | 11001000       | 144                | 64                      |
### Преобразование чисел из одной системы счисления в другую
Преобразовывать числа из восьмеричной в шестнадцатеричную или в двоичную систему счисления и обратно легко. Чтобы преобразовать двоичное число в восьмеричное, нужно разделить его на группы по три бита, причем три бита непосредственно слева от двоичной запятой формируют одну группу, следующие три бита слева от этой группы формируют вторую группу и т. д. Каждую группу по три бита можно преобразовать в один восьмеричный разряд со значением от 0 до 7. Чтобы дополнить группу до трех битов, нужно спереди приписать один или два нуля.
Преобразование из восьмеричной системы в двоичную тоже тривиально. Каждый восьмеричный разряд просто заменяется эквивалентным 3-разрядным числом. Преобразование из шестнадцатеричной в двоичную систему по сути сходно с преобразованием из восьмеричной в двоичную систему, только каждый шестнадцатеричный разряд соответствует группе из четырех битов, а не из трех.
Преобразование десятичных чисел в двоичные можно совершать двумя разными способами. Первый способ непосредственно вытекает из определения двоичных чисел. Самая большая степень двойки, меньшая, чем число, вычитается из этого числа. Та же операция проделывается с полученной разностью. Когда число разложено по степеням двойки, двоичное число может быть получено следующим образом. Единички ставятся в тех позициях, которые соответствуют полученным степеням двойки, а нули – во всех остальных позициях.
![[Pasted image 20251109151100.png]]
Второй способ (подходящий только для целых чисел) – деление на 2. Частное записывается непосредственно под исходным числом, а остаток (0 или 1) записывается рядом с частным. То же проделывается с полученным частным. Процесс повторяется до тех пор, пока не останется 0. В результате должно получиться две колонки чисел – частных и остатков. Двоичное число можно считать из колонки остатков снизу вверх.
![[Pasted image 20251109151116.png]]
Двоичные числа можно преобразовывать в десятичные двумя способами. Первый способ – суммирование степеней двойки, которые соответствуют битам 1 в двоичном числе. Например: $$10110_{(2)} = 24 + 22 + 21 = 16 + 4 + 2 = 22_{(10)}$$
Во втором способе двоичное число записывается вертикально по одному биту в строке, крайний левый бит находится внизу. Самая нижняя строка – это строка 1, затем идет строка 2 и т. д. Десятичное число строится напротив этой колонки. Сначала обозначим строку 1. Элемент строки n состоит из удвоенного элемента строки n – 1 плюс бит строки $n$ (0 или 1). Элемент, полученный в самой верхней строке, и будет ответом.
![[Pasted image 20251109151439.png]]
### Отрицательные двоичные числа
На протяжении всей истории цифровых компьютеров для представления отрицательных чисел использовались 3 различные системы. Первая из них называется системой **со знаком**. В такой системе крайний левый бит – это знаковый бит (0 – плюс, 1 – минус), а оставшиеся биты показывают абсолютное значение числа.

Во второй системе, которая называется **дополнением до единицы**, тоже присутствует знаковый бит (0 – плюс, 1 – минус). Чтобы сделать число отрицательным, нужно заменить каждую единицу нулем и каждый ноль единицей. Это относится и к знаковому биту. Система дополнения до единицы уже устарела.

Третья система, дополнение до двух, содержит знаковый бит (0 – плюс, 1 – минус). Отрицание числа происходит в два этапа. Сначала каждая единица меняется на ноль, а каждый ноль – на единицу (как и в системе дополнения до единицы). Затем к полученному результату прибавляется единица. Двоичное сложение происходит точно так же, как и десятичное, только перенос совершается в том случае, если сумма больше 1, а не больше 9. Например, рассмотрим преобразование числа 6 в форму с дополнением до двух:
- Число +6 $$00000110$$
- Число –6 в системе с дополнением до единицы $$11111001$$
- Число –6 в системе с дополнением до двух $$11111010$$
Если потребуется выполнить перенос из крайнего левого бита, он просто отбрасывается.
В четвертой системе, которая для m-разрядных чисел называется **системой со смещением на $2m–1$**, число представляется как сумма этого числа и $2m–1$. Например, для 8-разрядного числа ($m = 8$) – это система со смещением на 128, в ней число сохраняется в виде суммы исходного числа и 128. Следовательно, –3 превращается в $–3 + 128 = 125$, и это число (–3) представляется 8-разрядным двоичным числом 125 ($01111101$). Числа от –128 до +127 выражаются числами от 0 до 255 (все их можно записать в виде 8-разрядного положительного числа). Отметим, что эта система соответствует системе с дополнением до двух с обращенным знаковым битом.
## Двоичная арифметика
Сложение двух двоичных чисел начинается с крайнего правого бита. Суммируются соответствующие биты в первом и втором слагаемых. Перенос совершается на одну позицию влево, как и в десятичной арифметике. В арифметике с дополнением до единицы бит переноса после сложения крайних левых битов прибавляется к крайнему правому биту. Этот процесс называется циклическим переносом. В арифметике с дополнением до двух бит переноса, полученный в результате сложения крайних левых битов, просто отбрасывается.
![[Pasted image 20251109152041.png]]
Если первое и второе слагаемые имеют противоположные знаки, ошибки переполнения не произойдет. Если они имеют одинаковые знаки, а результат – противоположный знак, значит, произошла ошибка переполнения и результат неверен. И в арифметике с дополнением до единицы, и в арифметике с дополнением до двух переполнение происходит тогда и только тогда, когда перенос в знаковый бит отличается от переноса из знакового бита. В большинстве компьютеров перенос из знакового бита сохраняется, но перенос в знаковый бит из результата не виден, поэтому обычно вводится специальный бит переполнения.
## Числа с плавающей точкой
Как мы уже с вами видели компьютер немного странно работает с числами с плавающей точкой. Если мы не используем округление при сложении двух вещественных чисел, то вместо очевидного как нам кажется результата, мы получаем какую-то непонятную примесь в конце числа. Именно поэтому нам надо отдельно рассмотреть как компьютер хранит такие числа.
![[Pasted image 20251109152715.png]]

Как и любое другое число, число с плавающей точкой должно храниться в памяти компьютера в двоичном виде. Но первое чем она отличается от всех других чисел – это присутствие злополучной точки для которой просто нет места среди единиц и нулей.
![[Pasted image 20251109153141.png]]
Точку нельзя представить цифрой, а значит, очевидно, что здесь как и случае с отрицательными числами нужна какая-то специальная форма хранения, в которую мы сможем перевести исходное число и из которой мы сможем его восстановить.
Долгое время различные производители компьютеров по разному хранили такие числа в связи с отсутствием стандартизации. Создавался хаос как и в случае с кодировками. За решение этой проблемы взялся **IEEE (Institute of Electrical and Electronics Engineers)** – Институт инженеров электротехники и электроники. Их задачей было разработать некую модель, которой бы придерживались все разработчики аппаратного обеспечения. В итоге появился стандарт **IEEE-754** описывающий способ хранения чисел с плавающей точкой.
Чтобы процессоры **x86** могли выполнять операции на числами с плавающей точкой в процессоры моделей с 86 по 386 был добавлен отдельный модуль, который получил название сопроцессор **FPU (floating point unit)**.
![[Pasted image 20251109154109.png]]
В более поздних моделях процессоров этот модуль интегрировали в сам процессор, а после появления ряда технологий от компаний Intel и AMD появилась возможность работать с числами с плавающей точкой без сопроцессоров.
Чтобы понять все проблемы дробных чисел нам понадобится вручную перевести их из десятичной системы счисления в двоичную, после чего разберем как все это дело сохраняется в памяти согласно нашему стандарту.
Возьмем число $7.25$ и разделим его на две части: до точки и после точки. Иными словами выделяем целую и дробную часть. Как переводить целые числа двоичную систему счисления путем деления с остатком мы уже разбирали раньше. Но как перевести в двоичный вид дробную часть?
![[Pasted image 20251109154722.png]]
Если целую часть мы делим на 2 и, затем, собрав остаток в обратном порядке, мы получаем число двоичном виде. То дробную часть мы, наоборот, должны умножать на 2 до тех пор пока в ответе у дробной части не получится 0. У каждого последующего ответы мы берем только его дробную часть и продолжаем умножение. В конце, то что стало с целой частью, мы склеиваем и получаем дробную часть числа представленную двоичном виде. Сейчас неважно положительное это было число или отрицательное просто выполняем перевод не затрагивая знак числа.
![[Pasted image 20251109155106.png]]
И так мы получили число с плавающей точкой представлены в двоичном виде, но что дальше?
Стандарт предлагает хранить число в так называемой экспоненциальной записи, которая в нашем случае будет иметь следующий вид, где $S$ – это **знак** числа, $M$ – это **мантисса**, $E$ – **экспонента**. Именно эти три числа и будут храниться в памяти. Но чтобы не запутаться во всех этих терминах, проще будет называть мантису – дробной частью числа, перед которой всегда идет только 1 единица. А экспоненту – степенью основания, которое равняется количеству цифр на которые точка была смещена относительно своего начального положения.
![[Pasted image 20251109155521.png]]
Звучит это всё сложно, но по факту смысл всего этого в том, что любое наше число, которое получилось в результате перевода нужно подогнать под эту форму.
Чтобы это сделать нужно просто сдвинуть точку до тех пор пока слева от нее не останется только одна $1$. В нашем случае точку необходимо сместить на два знака влево.
![[Pasted image 20251109181619.png]]
Поэтому получившееся число мы умножаем на $10^2$. То есть каково было смещение точки, такой и будет ее степень. Если бы пришлось сместить точку на 3 знака влево, мы бы написали $10^3$, а если бы у нас было число наподобие такого, где спереди идут $0$. То мы бы сдвигали точку вправо пока не дошли бы до $1$. И так как мы сдвигали в противоположную сторону, то степень будет со знаком минус: $10^{-3}$.
![[Pasted image 20251109181704.png]]
Для сохранения такого числа стандарт предлагает нам сразу несколько форматов: с **одинарной**, **двойной** и **четырехкратной** точностью. Которые соответствуют 32, 64 и 128 битам. Помимо этого ещё используется 80 битный формат – формат расширенной двойной точности.
![[Pasted image 20251109182250.png]]
Все они одинаковы с точки зрения данных, но разные по количеству бит, которые предоставляются для хранения. И возможности использования каждого из этих форматов будет зависеть от компилятора конкретного языка и от аппаратного обеспечения.
Так, например, тип данных **float** соответствует одинарной точности, тип **double** – двойной. А такой тип данных как **long double** может относиться либо к расширенной точности, либо к четырехкратной, либо вообще может являться синонимом типа **double**. Рассмотрим формат на 32 бита, чтобы ничего не усложнять и посмотрим как нам записать в них наше получившееся число.
![[Pasted image 20251109182639.png]]

И так 32 бита разбиваются на три части: самый бит отводятся под **бит знака**. $0$ означает, что число положительное, а $1$, соответственно, отрицательное. Подобное мы уже видели когда рассматривали, как хранятся целые числа в памяти, но дальше всё кардинально отличается.
![[Pasted image 20251109183450.png]]
Во-первых, целая часть двоичного числа всегда будет равна единице, потому что мы подгоняем его под экспоненциальную форму записи. И если там будет всегда $1$, то зачем ее вообще хранить в памяти.
Поэтому целая часть в памяти не сохраняется. К тому же это повышает точность нашего числа на один бит.
![[Pasted image 20251109183800.png]]
Далее идет **дробная часть** которой отводится 23 бита в младшей части 32 бит. И сохраняется она относительно этих 23 бит с левой стороны все свободное место справа заполняется $0$. Так как эти нули ничего не значат, то есть никак не влияют на само число. Например $0,50$ это то же самое, что и $0,5$ то есть последний $0$ с правой стороны никак не влияет на само число и это же правило действительно и для двоичного числа.

Осталось разобраться с **основанием в степени**. Как и в случае с целой частью основание будет всегда равно 10. Поэтому хранить мы его тоже не будем, а вот для степени выделяются оставшиеся 8 бит между **битом знака** и **дробной частью**. И чтобы было понятно 8 бит – это $2^8$ или 256 значений. Т.е. в теории мы можем записать число точка, которого будет смещена на 255 знаков. Чтобы сохранить степень надо как-то учесть её знак, а ещё одного бита для знака степени тут не предусмотрено. Увы как с целыми числами поступить не получиться, поскольку нельзя выделить дополнительный бит или использовать существующий бит знака.
![[Pasted image 20251109185554.png]]
В данном случае решили хранить как положительные так и отрицательные степени относительно числа 127. То есть взяли 8-битный диапазон чисел от 0 до 255, в середине которого расположено число 127 и сместили степень либо в меньшую, либо в большую сторону относительно этого числа путем сложения степени с числом 127.
![[Pasted image 20251109190058.png]]
Говоря более простым языком, мы берем степень и прибавляем к ней 127. И если число было отрицательным, то итоговое значение будет меньше 127, а если число было положительно, то будет больше 127. Все отрицательные степени будут слева, а все положительные – справа.
![[Pasted image 20251109201134.png]]
На самом деле нам доступно для сохранения чисел не 255, а 254. По той причине, что стандартом предусмотрено еще их хранение специальных значений, которые отличаются от обычных чисел тем, что в степени у них будут храниться все единицы или число 255.
![[Pasted image 20251109201619.png]]
Так например, если в знаке стоит $0$ в степени стоят все единицы, а в дробной части все нули то такое число означает $+\infty$. Если все тоже самое, но в знаке $1$, значит, $-\infty$. И она подчиняется обычным математическим правилам. То есть, например, число $+\infty$ равняется $\infty$. Помимо этого входе вычисления могут возникнуть ошибки и результатом вычисления может оказаться несуществующие число. Например разделив $\infty$ на $\infty$ или сделав извлечения корня из отрицательного числа.
![[Pasted image 20251109201934.png]]
Несуществующие число по другому называют **NaN (not a number)** и для его записи в степени ставятся все $1$, биты дробной части могут содержать в себе любое число отличное от нуля, а бит знаков в таком числе просто не имеет никакого значения.

Мы успешно записали наше число памяти, а теперь попробуем восстановить его обратно в десятичную систему счисления, выполнив шаги в обратном порядке, используя все ту же форму в экспоненциальной записи.
![[Pasted image 20251109202317.png]]
Для начала возьмем дробную часть числа и прибавим к ней единицу, напомню, что она изначально там была просто мы договорились её не хранить, так как она будет там всегда. Тоже самое касается и основание системы счисления, которую мы не храним, но помним, что она там должна быть.
![[Pasted image 20251109202449.png]]
Поэтому умножаем получившееся число на 10 в степени, которая у нас сохранена только предварительно из нее нужно вычесть 127. Последнее что нужно сделать это вернуть знак нашему числу. Для этого мы умножаем его на минус 1 в степени бита знака.
![[Pasted image 20251109202649.png]]
У нас получилось двоичное число, которое осталось перевести в десятичный вид.
![[Pasted image 20251109202944.png]]
У числа до точки разряды начинаются от нуля с правой стороны, то есть как и любого другого целого числа. У дробной части разряды идут от $-1$ начиная слева и все что нужно сделать это умножить основании двоичной системы счисления, представленную в десятичном виде в степени разряда, на цифру этого разряда. Далее все это складываемый и как ни странно получаем число в десятичной системе счисления.
Вроде всё кажется хорошо и мы без проблем перевели число из одного вида в другой и обратно. Но в реальности всё не так радужно.
![[Pasted image 20251109203231.png]]
Давайте возьмем к примеру число $5.9$. В момент перевода этого числа в двоичный вид, мы понимаем, что как-то особо оно и не переводится. Наши ответы начинают повторяться до бесконечности и всё что нам остается сделать, так это записать данное число в периоде.
![[Pasted image 20251109203435.png]]
Получается, что некоторые дроби в десятичной системе счисления просто невозможно представить в двоичном виде, так как они становятся бесконечными.
Как можно бесконечность уместить в 23 бита? Только путем отсечения части числа, которая в 23 бита не помещается. Отсечение части числа будет приводить к потере точности, поэтому восстановить исходный вид нам уже не удастся.
![[Pasted image 20251109203919.png]]
Это число также приводится к экспоненциальной записи и точно также записывается бит знака и степень. А в 23 бита отведенные для дробной части мы записываем столько бит сколько туда помещается.
Теперь по нашей формуле попробуем перевести это число обратно в десятичный вид. И вот то, что мы никак не ожидали увидеть. Наше исходное число $5.9$ превращается в $5.89999961$.
![[Pasted image 20251109204314.png]]
Компьютеры умеют работать с дробными числами лишь до определенной точности.
## Кодирование текстовой информации
Компьютер – это машина работающая с числами. Человек – это машина работающая с текстом. Любой текстовый символ можно каким-то конкретным числом и таким образом хранить его в памяти. Так оно и происходит.
![[Pasted image 20251109213717.png]]
Когда компьютер читает код символа из памяти, он видит что этот код соответствует букве $а$ и выводит букву $а$ на экран. И чтобы это число отображалось на экране именно таким символом, все компьютеры на планете должны использовать один и тот же стандарт.
![[Pasted image 20251109214026.png]]
Некие таблицы в которых указано какому символу какое число соответствует. Звучит просто, но на деле развитие этих стандартов сопровождалось чередой глобальных проблем, которые не были заранее предусмотрены. Работа со строками – это одна из самых важных тем в программирование, потому что строки там основной вид информации, с которым мы работаем.
![[Pasted image 20251109214231.png]]
Каждый символ в компьютере представляется в виде целого числа. Это число называется **кодом символа**. Для того чтобы мы могли использовать текстовую информацию на разных компьютерах, они должны поддерживать одни и те же коды символов. Иначе один и тот же символ будет отображаться по разному на разных компьютерах и и обмен информации станет невозможно. Отсюда следует что коды символов нужно стандартизировать, поэтому и были разработаны стандарты.
### ASCII
Компьютерные технологии развивались преимущественно в США и в ходе экспериментов там был создан стандарт, подходящий для английского языка. Стандарт получил название **ASCII (America standard code for information interchange)**. В ходе разработки стандарта не учитывались потребности в символах других языков, поэтому на каждый **ASCII** символ было выделено всего 7 бит.
![[Pasted image 20251109214939.png]]
В 7 бит помещаются числа от 0 до 127 и таким образом всего можно было закодировать 128 символов. Но минимально ячейка памяти, которую можно адресовать, составляет 8 бит поэтому для хранения каждого **ASCII** символом используется один байт который способен хранить в себе 256 значений. Но так как символы 7 битные, то 1 бит в этой ячейке всегда будет оставаться пустым.
![[Pasted image 20251109215233.png]]
Таким образом остается место для 128 значений, которые никем не используются. В этом стандарте первые 31 символ являются управляющими. Они не отображаются на экране, а используется для выполнения каких-либо действий. Например, это символы перевода строки и возврата каретки. Все последующие символы имеют внешний вид и отображаются на экране.

Пока программное обеспечение преимущественно выпускалось для англоязычной аудитории все было хорошо, но время шло и компьютеры стали распространяться по всему миру. Появилась необходимость выпускать программное обеспечение не только для англоязычной аудитории, но и для остальных пользователей, которые по-английски не говорят.
Во многих европейских странах есть буквы, которых нет в английском алфавите, а в таких языках как русский, китайский и арабский – алфавит вообще кардинально отличается. Разумеется, что используя только 7 бит, невозможно охватить все необходимые символы, включая десятки тысяч иероглифов разных языков. Нужно было как-то решить проблему с нехваткой символов и так как оставалось еще сто 128 мест внутри, стандарт ASCII решили расширить введя **кодовые страницы**.
### Кодовая страница
**Кодовая страница** – это набор из 256 символов для конкретного языка. Первые 128 символов полностью совпадали символами **ASCII**, остальные 128 символов в разных кодовых страницах были разные в зависимости от языка для которого предназначалась эта страница. В каждой стране начали добавлять символы своего алфавита в свободные 128 мест, причем каждый добавлял их как хотел.
Например в одной и той же стране могли сделать сразу по нескольку разных кодовых страниц с символами в разном порядке. Не было никакой упорядоченности и это выливалось в то, что если два разных компьютерах использовали две разные кодовые страниц, то текст одного компьютера на втором был просто не читаем.
В любом случае кодовой страницы имея в своем распоряжении всего 256 символов не могли охватить все возможные языки. Плюс проблема усугубилась, когда появился Интернет. Люди начали обмениваться информацией, используя разные кодовые страницы, и отсутствие какой-либо мировой стандартизации привело все к хаосу.
### ANSI
Параллельно с развитием компьютеров стали развиваться и операционные системы. На первый план вышла операционная система Windows. Она взяла за основу стандарта **ASCII**, расширила его своими символами и назвала **ANSI (American National Standards Institute)**. На основе **ANSI** начали создаваться кодовые страницы под названием **windows-125X**, где X – номер страны. Например, для кириллицы это было **windows-1251**.
![[Pasted image 20251109222507.png]]
Вскоре Windows точно также распространилась по всему миру, дойдя до Азии, где кодовыми страницами проблему было не решить. 256 значений в байте против 1000 иероглифов в алфавитах. Windows попытались придумать новый способ кодирования, где для хранения одних символов использовался бы 1 байт, а для других 2 и это решало отдельно взятую проблему с кодированием символов, но это не решало общую проблему стандартизации.
![[Pasted image 20251109222656.png]]
Каждый продолжал пользоваться тем, чем хотел и, пересылая по интернету текст в одной кодировке, он не мог корректно отображаться в другой кодировке на компьютере получателя.
### Unicode
Нужно было придумать единый стандарт кодирование символов для всех языков мира. Таким образом появился международный стандарт **UNICODE**, призванный заменить собой **ASCII**. Его особенность заключалась в том, что если в **ASCII** под каждый символ выделялась 7 бит и максимальное количество символов составляло – 128, то для **UNICODE** не было никаких ограничений на количество символов и каждый его символ представлялся минимум 2 байтами.
![[Pasted image 20251109223333.png]]
И в первой версии **UNICODE** все необходимые символы как раз умещались в эти 2 байта. То есть их размер не превышал 65536 кодов. Каждому символу по-прежнему соответствовал определенный числовой код, но при этом для записи кода буквы было принято использовать специальный 16-ричный формат `U + код символа`.
![[Pasted image 20251109223508.png]]
Второй важный момент был в том, что первый 128 символов полностью совпадали с **ASCII**. Со временем появлялись новые символы, иконки, смайлики и **UNICODE** стал расширяться.
Что продолжается и по сей день. На текущий момент в 17 версии уже почти 160 000 символов. **UNICODE** решил проблемы с нехваткой символов и казалось, что проблема на этом должны были закончиться.

Однако его появление привело к другим проблемам, которые также предстояло решать.
![[Pasted image 20251109224052.png]]
Первая проблема заключалась в том, что любые символы в **юникоде** занимали минимум 16 бит, что воспринималось как расточительство памяти в англоязычных странах, в  которых можно было использовать **ASCII**. Зачем использовать 16 бит для символов английского алфавита когда можно использовать 7 битный ASCII код и экономить память в два раза. По этой причине многие продолжали пользоваться **ASCII**, игнорируя **юникод**.
Другая проблема была связана с разным расположение байт памяти компьютера. Байты могут располагаться в прямой и обратной последовательности. Так называемые **big-endian** и **little-endian**. Следовательно в разных компьютерах строки в памяти тоже располагались в разном порядке. И при пересылке текста с одного компьютера на другой было неизвестно в каком порядке передается текст и как его сохранять в памяти. Раньше такой проблемы не было потому, что для хранения ASCII символов использовался один байт и было не важно в каком порядке они передавались.
![[Pasted image 20251109224824.png]]
В юникоде каждый символ состоит из 2 байт и какой из этих байтов является началом, а какой концом попробуй разбери. Например, строка `hi` в юникоде выглядит как `00 68 00 69`. Cимвол `H` имеет 2 байтовый код `00 68` и в **BE** он будет храниться в точно таком же порядке. Но в **LE** байты в каждом символе поменяются местами и получится `68 00`. В итоге при чтении этого символа из памяти такой код в юникоде будет выглядеть не как буква `H`, f как такой иероглиф.
![[Pasted image 20251109224915.png]]
Чтобы решить проблему с порядком хранения строк в памяти появляются **кодировки** – правила, описывающие хранение **unicode** символов в памяти.
### UCS-2 и BOM-байты
![[Pasted image 20251109225650.png]]
Одна из первых кодировок получила название **UCS-2 (Universal coded character set)**. Эта кодировка хранила все символы, используя фиксированную длину в 2 байта. Отсюда и цифра 2 в названии.
В этой кодировке проблему с порядком хранения байтов решили следующим образом. Перед отправкой или сохранением файла в самое начало добавлялось 2-х байтовое число, которое получило название **BOM (Unicode byte order mark)**. Если использовался **BE**, то эти байты шли в прямой последовательности, если **LE** то они менялись местами. Получавший компьютер считывал эти первые 2 байта и понимал в какой последовательности расположена строка. В результате такого решения **unicode** строка стала занимать еще больше памяти, чем было. Чтобы угодить всем нужна была более универсальная кодировка, которая могла бы решать проблему как с хранением порядка символов, так и с экономией места.
### UTF-8
Такой кодировкой стала **UTF-8 (Unicode transformation format)**. Именно она впоследствии стала доминирующей кодировкой в интернете. Решение заключалось в том, чтобы сделать коды символов не фиксированной, а переменной длины от 1 до 4 байт.
Восьмерка в названии стоит как раз потому, что минимальная длина, в которой может быть представлен символ составляет 8 бит. И важный момент здесь в том, что первый 128 символов **юникода** полностью совпадают с первыми 128 символами **ASCII**, поэтому в кодировке **utf-8** эти первые 128 символов хранятся в памяти используя только 1 байт. То чего все так долго ждали. Остальные символы от 128 и дальше занимают в памяти от 2 до 4 байт. Чтобы понять каким образом решилась проблема с BOM-байтами посмотрим, как utf-8 кодирует символы разной длины.
![[Pasted image 20251109230425.png]]
Существует четыре маски для кодов разной длины и, например, длина первых 128 кодов помещаются в диапазонах 0 до 7 бит включительно поэтому храниться они будут согласно маске и их длины. В начале байта ставится 0, далее записывается код символа и в таком виде он сохраняется в памяти.
Далее все по аналогии, для кода соответствующей длины будет использоваться и соответствующая маска. В самом начале байтов ставятся специальные метки и в таком виде все это сохраняется в памяти. Эти метки определяют сколько байт нужно прочитать из памяти. Если в начале байта стоит последовательность `110` значит нужно прочитать 2 байта, если `1110` значит 3 байта и так далее.
По этим же самым меткам можно было определить где находится начало символа, благодаря чему в этой кодировке можно было не использовать BOM-байты. Так называемый **utf-8 без BOM**. То есть вначале символом всегда будет одно из этих значений: `0`, `110`, `1110`, `11110`.
Если в компьютере используется **BE**, то он увидит одно из этих чисел, например, `1110` и поймет что это начало символом который состоит из 3 байт, читает 3 байта и идём дальше. Если используется **LE**, то 1 байт будет начинаться с `10`. И в таком случае компьютер поймет, что надо идти назад до тех пор, пока не будет найдена одна из 4 последовательностей, с которой символ должен начинаться.
![[Pasted image 20251109231411.png]]
Рассмотрим пример для символов `П` в **юникоде** код этого символа имеет вид `U+041F` в шестнадцатеричной системе счисления и переведя его двоичный вид мы получим код длиной 11 бит. Для кода длиной в 11 бит используется вторая маска. В начало обоих байтов подставляем метки и заполняем оставшееся место кодом символом. Переведя этот код обратно в 16-ричный формат мы получаем `D0 9F`. И в таблицы кодов **utf-8** мы можем найти этот символ. Это будет наша буква `П`.
Казалось бы **utf-8** окончательно решил все проблемы, но его нововведения в виде динамического размера символов привело к увеличению времени обработки строк. Каждый символ нужно было проверять на его длину. И таким образом код стал работать медленнее. Из-за этой медлительности многие языки программирования продолжали пользоваться старой кодировкой, где на каждый символ приходилось по 2 байта.
Но как ни странно спустя время оказалось, что этих 2 байт не хватает для хранения всех требуемых символов. Поэтому развитие кодировок продолжилось и на свет появился **UTF-16**. А ещё чуть позже **UTF-32**.
## Как устроен PNG-файл
Как-то раз Жак Фреско в одной из своих историй упомянул цитату: "Мы не можем видеть вещи такими какие они есть мы их видим только так как нам позволяют наши органы восприятия". Пожалуй это касается абсолютно всего с чем мы взаимодействуем и даже простая картинка на экране монитора большинстве случаев всего лишь мерцающие светодиоды от которых мы успешно абстрагируемся. Физический мир в том виде в котором мы привыкли его видеть в последние годы все больше и больше уходит на второй план освобождая место миру виртуальному в котором все что мы видим выглядит совершенно не так как нам кажется.
Изображения которое мы ежедневно листаем социальных сетях выглядящие монолитным рисунком хранящимся глубоко в памяти при максимальном увеличении моментально рассыпаются на миллионы мелких квадратиков каждый из которых изначально получил название **picture element**, которое впоследствии сократят просто до **pix** (пиксель) – наименьший неделимый элемент двумерного растрового изображения, обладающий определенным цветом, яркостью и в некоторых случаях определенной прозрачностью. 
![[Pasted image 20251028160003.png]]
Слово *растровое* здесь означает что изображение представляет собой фиксированный набор пикселей. Благодаря чему мы можем максимально приблизится к каждому из них и благодаря чему изменение размера изображения ведет к потере его качества.
В отличие например от *векторной графики* в которой изображение строится на основе математического описания примитивов, вроде точек, линий, кругов и и т.д. Происходит постоянный перерасчет, изображение качество не теряет, но позволяет создавать очень ограниченный набор изображений не требующих сложных переходов вроде логотипов иконок и так далее.
Растровая графика позволяет нам создать практически любое изображение, что делает именно ее самой используемой графикой в самых различных сферах. Каждое растровое изображение может состоять как всего из одного пикселя, символизирующего собой просто точку, так и сотен, тысяч или даже миллионов пикселей самых различных цветов. На самом деле каждый пиксель, каким бы квадратным он нам не представлялся в своей конечной форме на мониторе, в памяти компьютера представляет собой определенный набор данных, описывающий один конкретный цвет согласно цветовой модели, которую мы будем использовать.
**Цветовая модель** – это такая математическая модель которая позволяет записать цвет видим одного либо нескольких каналов. Каждый канал записывается в виде одного числа представляющего собой цветовую координату в неком цветовом пространстве.
![[Pasted image 20251028160617.png]]
Например, полутоновое изображения являются всего лишь оттенками серого поэтому для описания такого цвета нам хватит всего 1 канала, который представляет собой уровень яркости белого цвета. Это означает что для описания каждого такого одноканального пикселя понадобится всего одно число. Для описания полноцветных изображений понадобится уже не 1, а сразу 3 канала обозначающие оттенки трех цветов: *красного*, *зеленого* и *синего*. Отсюда и сокращенное название этой цветовой модели **RGB**. Cоответственно каждый пиксель здесь описываться будет тоже не одним а сразу тремя числами.
![[Pasted image 20251028160849.png]]
Логично предположить, что чем больше количество бит будет отводится под запись каждого числа сохраняясь памяти, тем больший диапазон значений мы сможем охватить для описания цвета, тем больше будет размерность цветового пространства и тем большее количество цветов мы сможем использовать.
![[Pasted image 20251028161030.png]]
По другому эта характеристика называется **глубиной цвета**. Например, если бы для описания цвета каждого пикселя отводился всего 1 бит, то мы бы смогли использовать всего 2 цвета. Два бита предоставили бы нам в распоряжении 4 цвета, а к примеру 8 бит дадут нам уже к 256 различных цветов. Что все равно крайне мало по сравнению с тем набором разнообразных оттенков, которые наш глаз привык видеть в реальном мире.
![[Pasted image 20251028161216.png]]
По этой причине увеличения глубины цвета, то есть увеличение количества бит способны сделать картинку намного реалистичней в плане ее цветовой гаммы. Например, RGB изображение с 24-битной глубиной позволяет выделить на каждый из трех каналов по 8 бит что в сумме дает нам более 16 миллионов всевозможных цветов и даже несмотря на такое внушительное количество уже сейчас существует еще большие значения. Например, 48 бит позволяющий использовать 281 триллион цветов что далеко за пределами того что может распознать человеческий глаз.
![[Pasted image 20251028161343.png]]
![[Pasted image 20251028161416.png]]
**Цветовая модель** или проще говоря тип цвета вместе с его глубиной напрямую влияют на размер всего изображения. К примеру в картинке размером 1440х2160 пикселей мы получаем 9,331,200. И это не общее количество пикселей в изображении это количество байт которое данное изображение занимает в памяти.
Переводя в более понятную всем величину это получается в районе 9 МБ, что кажется не таким уж большим по сравнению с тем железом которые сейчас продается чья емкость давно уже измеряется в терабайтах.
![[Pasted image 20251028161808.png]]
Однако, во-первых изображение может быть намного больше нашего примера. Во-вторых если бы каждое изображение, каждое видео и каждая аудио хранилась в компьютере в том виде, в котором оно есть изначально, то ни к чему хорошему это бы точно не привело. Хорошо бы все эти байты утрамбовать так, чтобы количество их памяти стало меньше, но сами данные при этом никуда не девались. Такой способ есть и называются он **компрессией** или **сжатием**.
Алгоритмов для этих целей существует довольно много, но условно все их можно поделить на две большие группы. Это алгоритмы **сжатия с потерей данных** и **без потери**. Разница в том, что в первом случае данные пропущенные через специальный алгоритм частично будут безвозвратно утеряны. Это означает, что восстановить изображение один-в-один таким каким оно была до сжатия невозможно. Да, конечно, при этом с технической точки зрения качество изображения ухудшится, но человеческому глазу в большинстве случаев будет абсолютно все равно, что часть исходных цветовых данных отсутствуют. Он просто не способен замечать такое их количество. **Сжатие без потерь**, соответственно, не так хорошо экономит место, но позволяет из сжатых байтов воссоздать изображение в точности таким, каким оно и было. И чтобы программа декодер, в который мы попытаемся открыть наши изображения смогла корректно это сделать, должна как минимум владеть всеми теми характеристиками, о которых мы говорили все это время. Поэтому все плюс сами данные должны быть запакованы в *специальный формат*, взаимодействие с которым будет стандартизировано и полностью будет описано в документации.

Один из таких форматов получил название **PNG**. Однако перед тем как начать о нем говорить стоит узнать его прошлое. Причина по которой он вообще возник и какие события этому предшествовали. Предшествовал ему такой всем известный формат, как **GIF** который впоследствии станет виновником в необходимости создания нового формата.
Растровый формат **GIF** появился в далеком 1987 году и предназначался он для передачи растровых изображений по сети. Общее количество цветов которое **GIF** мог использовать составляла всего 256, что для того времени было вполне нормально. Плюс этот формат позволял передавать изображение в своем изначальном виде, благодаря, сжатию без потерь через алгоритм который имел аббревиатуру **LZW**.
Он позволяет хорошо сжимать данные путем поиска одинаковых последовательностей, что достаточно эффективно когда много горизонтальной пикселей имеют одинаковый цвет.
![[Pasted image 20251028162857.png]]
Спустя два года формат немного модифицировали добавив к нему поддержку прозрачности и анимации. Правда прозрачность была бинарная, то есть пиксель может быть либо полностью прозрачным, либо наоборот полностью непрозрачным. Анимация же в свою очередь представляет собой последовательность из нескольких статичных кадров и информации о том, сколько времени длится каждый кадр.
Всё это продолжало неплохо работать вплоть до 28 декабря 1994 года, когда компания **Unisys corporation** решила запатентовать алгоритм сжатия **LZW**, что в свою очередь требовало лицензионных платежей под программное обеспечение поддерживающее **GIF**. Платить за использование доступной всем графики захотелось не всем. И спустя всего каких-то 7 дней (04 января 1995 года) было предложено создать новый формат, который бы мог распространяться в свободном доступе и который был бы точно не хуже, чем **GIF**. А спустя еще три недели было предложено несколько вариантов нового формата, получившего название **portable bitmap format**, который 23 января 1995 года был заменен на **portable network graphics** или привычное нам **PNG**, который иногда в шутку стали расшифровывать, как **PNG not GIF**.
01 октября 1996 вышла первая версия этого формата, которая и стала основным форматом для передачи графики по сети. Что же нового принес этот формат и действительно ли он смог во всем превзойти своего предшественника.
Во-первых **PNG** смог решить главную проблему сжатия данных, начав использовать не запатентованный алгоритм сжатия **DEFLATE**. Этот алгоритм тоже сжимает данные без потерь, сохраняя всю исходную информацию в отличие от того же **JPEG**, второго по популярности формата. Которым во время сжатия часть данных навсегда теряется из-за чего, собственно, страдает конечное качество картинки. Зато сам файл, хоть и не всегда, но будет занимать меньше места, чем тот же **PNG**.
![[Pasted image 20251028164307.png]]
На самом деле в этом и заключается основная фишка в существование столь многих форматов. **PNG** хорош там, где качество изображения важнее, чем его размер. И часто его редактирование в каких-нибудь графических редакторах дает свои плоды всегда оставляя качества на прежнем уровне. Второе важное отличие от **GIF**, поддержка полноцветной палитры. **PNG** также, как и **GIF** может продолжать использовать 8 бит для описания цвета пикселя, но может также использовать глубину и в 24 бита. Именно поэтому часто можно встретить такие названия как **PNG-8** или **PNG-24**.
Что касается прозрачности **PNG** в этом плане сильно превзошел **GIF**. Введя возможность использования, так называемого **альфа-канала**, позволяющего регулировать степень прозрачности пикселя. Данное нововведение в свою очередь порождает еще две новые цветовых модели. Это полутоновое изображение с прозрачностью и полноцветное изображение с прозрачностью называемая **RBGA**. Это означает что если раньше у нас, например, использовалась глубина 24 бита, где на каждый канал приходилось по 8 бит, то теперь появляется 4 канал **альфа**, на который тоже выделяется 8 бит.
![[Pasted image 20251028164804.png]]
 Однако стоит понимать, что на глубину цвета это никак не влияет. Количество цветов остается столько же сколько и было, только теперь появляются дополнительные 256 оттенков прозрачности для каждого пикселя, где 0 – это полностью прозрачный пиксель, а 255 – полностью непрозрачный.
![[Pasted image 20251028165046.png]]
Если во всем предыдущем **PNG**, как формат стал намного лучше, то в случае с анимацией **PNG** **GIF** превзойти не смог. Анимацию в этот формат решили просто не добавлять, что впоследствии станет причиной многих разногласий. Время от времени будут появляться безуспешные попытки создать расширенные форматы на основе **PNG**, которые бы могли поддерживать анимацию. Например 2001 году все теми же разработчиками **PNG** был опубликован формат **MNG**, в котором получилось реализовать поддержку анимаций, но не получилось заставить браузеры начать поддерживать этот новый формат. Дело в том, что формат **MNG** имеет структуру несовместимую с декодерами **PNG**. Из-за чего его очень сильно критиковали. 
Учтя эти ошибки в 2004 году разработчики **Mozilla Foundation** выпускают новый формат **APNG**, который кардинально структуры не меняет, а лишь расширяют уже существующую, что, наконец, нивелирует все проблемы с обратной совместимостью. То есть декодеры не поддерживающие **APNG** просто отобразят статичную картинку, как будто это самый обыкновенный формат **PNG**. Казалось бы такой формат можно было бы и утвердить, однако, на официальном уровне он снова был отвергнут. Что в принципе не сильно повлияло на его медленное, но уверенное и внедрение во все большее и большее количество браузеров и некоторых программ для просмотра изображений. Поэтому на текущий момент **APNG** вполне себе можно использовать практически во всех популярных браузерах.